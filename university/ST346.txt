Design Matrix Defintition of Nested Models	Consider two models<div><br></div><div>\[ M_1 : \mathbb{E}(Y) = X^{(1)}\beta^{(1)}\\ M_2 : \mathbb{E}(Y) = X^{(2)}\beta^{(2)} \]</div><div><br></div><div>\(M_1 \preceq M_2\) if there is a matrix \(C\) such that \(X^{(1)} = X^{(2)}C\). If \(C\) is invertible then the models are identical (though possibly parametrised differently).</div>	definition
Deviance	The deviance is the sum of squares evaluated at \(\hat\beta\).<div><br></div><div>\[S(\hat\beta) = \sum(y_i - x_i^T \hat\beta)^2 = \sum(y_i - \hat{y_i})^2 \]</div>	definition
Coefficient of Determination	\[R^2 = 1 - \frac{\text{Deviance}(M)}{\text{Deviance}(M_0)} =&nbsp;&nbsp;1 - \frac{\sum(y_i - \hat{y_i})^2}{\sum(y_i - \bar{y})^2}\]<div><br></div><div>In terms of sum of squares, we have</div><div><br></div><div>\[R^2 = 1 - \frac{\text{ResidSS}}{\text{TotalSS}} = \frac{\text{RegrSS}}{\text{TotalSS}}\]</div>	definition
Decomposition of Sum of Squares	\[\sum(y_i - \bar{y})^2 = \sum(y_i - \hat{y_i})^2 + \sum(\hat{y_i} - \bar{y})^2\]<div><br></div><div>In other words, TotalSS = ResidualSS + RegressionSS.</div>	result
Adjusted R-Squared (and Justification)	\[R^2_\text{adj} = 1 - \frac{\frac{1}{n-p}\text{ResidSS}}{\frac{1}{n-1}\text{TotalSS}}\]	definition
Mallows' \(C_p\)	Consider a model \(M \preceq M_\text{max}\),<div><br></div><div>\[C_p(M) = \frac{\text{Deviance}(M)}{s^2_\text{max}} + 2p -&nbsp; n\]</div><div><br></div><div>A smaller \(C_p\) is better.</div>	definition
LOOCV results	\[y_i - \hat{\beta_{(-i)}}^T = \hat{\varepsilon_i} + \frac{h_i \hat{\varepsilon_i}}{1 - h_i} = \hat{\varepsilon_i} + O(n^{-1})\]<div><br></div><div>\[\sum(y_i - \hat{\beta_{(-i)}}^T)^2 \overset{\mathbb{E}}\longrightarrow \text{Deviance} + 2\sigma^2p\]</div>	result
The Marginality Principle	If you include an interaction term in a linear model, you should also include the main effects. You should also be cautious when testing or interpreting effects of main effects.	definition
Martin's Advice on Interactions	<ul><li>Get the scale of dose-response relationship right first</li><li>Respect the marginality principle</li><li>At least variable in an interaction term should be a factor</li><li>Use the stratified parameterization</li><li>Do not look for interactions without good evidence</li><li>Do not go beyond two-way interations</li></ul>	definition
Akaike Information Criterion	\[\text{AIC} = -2\mathcal{l}(\hat\beta, \hat{\sigma^2}) + 2p\]<div><br></div><div>(\(\mathcal{l}\) is log-likelihood)</div>	definition
AIC for Normal Linear Model	<div>Fixed variance \(\sigma^2\):</div><div><br></div>\[\frac{\sum(y_i - \hat\beta^Tx_i)^2}{\sigma^2} + 2p\]<div><br></div><div>Unknown variance using MLE:</div><div><br></div><div>\[n\log(\text{ResidSS}) + 2p\]<br></div>	result
F Statistic for Comparison to Null Model	\[F = \frac{\frac{1}{p-1}\text{RegrSS}}{\frac{1}{n-p}\text{ResidSS}} = \frac{\frac{1}{p-1}\sum\left(\hat{y_i} - \bar{y}\right)^2}{\frac{1}{n-p_2}\sum\left(\bar{y} - y_i\right)^2}\sim F_{p-1, n-p}\]	result
F Statistic for Comparison of Nested Models	For \(M_1 \preceq M_2\),<div><br></div><div>\[F = \frac{\frac{1}{p_2-p_1}\sum\left(\hat{y_i}^{(1)} - \hat{y_i}^{(2)}\right)^2}{\frac{1}{n-p_2}\sum\left(\hat{y_i}^{(2)} - y_i\right)^2} \sim F_{p_2-p_1, n-p_2}\]</div>	result
Poisson Distribution Properties	\[p(y | \mu) = \frac{e^{-\mu}\mu^y}{y!} \\ Y_+ \sim \text{Pois}(\lambda T_+)\\M(t) = \exp(\mu(e^t-1)) \\ \hat\lambda = \frac{y}{T}\]	result
Binomial Distribution Properties	\[M(t) = \left(\mu e^t + 1 - \mu\right)^N\]	result
Exponential Dispersion Model	A random variable is distributed according to to an exponential dispersion model if we can write its probability density function as<div><br></div><div>\[ p(y | \theta, \phi) = a(y, \phi) \exp\left(\frac{\theta y - b(\theta)}{\phi}\right) \]</div><div><br></div><div>\(\theta \in \Theta \subseteq \mathbb{R}\) is called the canonical parameter and \(\phi &gt; 0\) is the dispersion parameter (which may be fixed or free).</div>	definition
Normal Distribution EDM parameters	\[\theta = \mu \\ \phi = \sigma^2 \\ b(\theta) = \frac{\theta ^ 2}{2}\]	result
Poisson Distribution EDM parameters	\[\theta = \log\mu \\ \phi = 1 \\ b(\theta) = e^\theta\]	result
Binomial Distribution EDM parameters	\[\theta = \text{logit}(\mu) = \log\left(\frac{\mu}{1-\mu}\right)\\ \phi = 1 \\ b(\theta) = N\log(1 + e^\theta) \]	result
Scaled Binomial Distribution	Let \(Y \sim \text{Bin}(N, \mu)\) and \(Z = \frac{Y}{N}\). Then \(Z \sim \text{ScaledBin}(N, \mu)\).	definition
Scaled Binomial Distribution EDM parameters	\[\theta = \text{logit}(\mu) = \log\left(\frac{\mu}{1-\mu}\right)\\ \phi = \frac{1}{N} \\ b(\theta) = \log(1 + e^\theta) \]	result
MGF of EDMs	\[M(t) = \exp\left(\frac{b(\theta + t\phi - b(\theta))}{\phi}\right)\]	result
Expectation and Variance of EDMs	(Calculated through CGF)<div><br></div><div>\[ \mathbb{E}(Y) = b'(\theta) \\ \mathbb{V}(Y) = \phi b''(\theta) \]</div><div><br></div><div>(Implies that \(\theta, \mu\) have a 1-1 correspondence)</div>	result
Variance Function	The variance function of an EDM is that function \(V\) satisfying \(\mathbb{V}(Y) = \phi V(\mu)\).	definition
Uniqueness of EDM	An EDM is uniquely defined by the relation \(\mathbb{V}(Y) = \phi V(\mu)\).<div><br></div><div>\[\theta(\mu) = \int\frac{1}{V(\mu)} d\mu\]</div>	definition
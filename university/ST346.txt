Log likelihood for EDM	\(l(\theta) = \frac{\overline{\rm Y}\theta - b(\theta)}{\frac{\phi}{n}}\)	
Sample mean distribution&nbsp;	\(\overline{Y}\) ~ EDM(\(\theta, \phi_i) \)	
MLE for \(\mu\) with no weights&nbsp;	\(\hat{\mu} = \overline{\rm Y} \)	
MLE of \(\mu\) with weights&nbsp;	\(\hat{\mu} = \frac{\Sigma_i \omega_i Y_i}{\Sigma_i \omega_i} \)	
Unit deviance&nbsp;	\(d(y,\mu) = 2[t(y,y) - t(y,\mu)], t(y,\mu) = \theta y - b(\theta) \)	
Total deviance&nbsp;	\(D(y,\mu) = \Sigma_{i=1}^N \omega_i d(y_i,\mu_i)\)	
Scaled Deviance&nbsp;	\(D^*(y,\mu) = \frac{D(y,\mu)}{\phi}\)	
EDM in dispersion form using unit deviance&nbsp;	\(p(y|\mu,\phi) = a^*(y,\phi)exp{\frac{-d(y,\mu)}{2\phi}}\)	
Saddlepoint approximation&nbsp;	\(\tilde{p}(\mu,\phi) = \frac{1}{\sqrt{2\pi\phi V(y)}}exp{\frac{-d(y,\mu)}{2\phi}}\)	
Approximating the pdf using Saddlepoint Approximation&nbsp;	\(~{p}(y) = \frac{1}{\sqrt{2\pi K''(\hat{t})}}exp{(K(\hat{t})-\hat{t}y)}\) where \(\hat{t}\) solves \(K'(\hat{t})=y\)	
Distribution of \(\frac{d(y,\mu)}{\phi}\)	&nbsp;\(\frac{d(y,\mu)}{\phi} \text{~} \chi_1^2\)	
Conditions for Saddlepoint approximation	Poisson: \(\mu \geq 3\)<div>Binomial: \(m\mu \geq 3\) and \(m(1-\mu) \geq 3\)</div><div>Gamma: \(\phi \leq \frac{1}{3} \) (shape \(\geq 3\))</div>	
Random component of EDM&nbsp;	\(Y_i \text{~} EDM (\mu_i, \frac{\phi}{\omega_i})\)	
Systematic Component: Linear Predictor&nbsp;	\(\eta_i = \beta^T x_i + \delta_i\)	
Systematic Component: Link funciton	\(g(\mu_i) = \eta_i\)	
Response Residual&nbsp;	\(r_i = y_i - \hat{\mu}_i\)	
Pearson Residuals and aymptomatic behaviour&nbsp;	\(r_i = \frac{Y_i - \hat{\mu}_i}{{\sqrt{\frac{V(\hat{\mu}_i)}{\omega_i}}}} \)<div>\(\mathbb{E}(r_i) \to 0\)</div><div>\(Var(r_i) \to \phi \)</div>	
Deviance Residual	\(r_i = sign(y_i - \hat{\mu}_i)\sqrt{\omega_i d(y_i,\hat{\mu}_i)}\)	
Working Residual	\(r_i = z_i - x_i^T\hat{\beta} = (y_i - \hat{\mu}_i)g'(\hat{\mu}_i)\)	
\(\mathbb{E}(z_i | \beta, \phi) \) from IWLS, \(z_i\) working observations&nbsp;	\(x_i^T \beta \)	
\(Var(z_i | \underline{\beta},\phi)\) from IWLS, \(z_i\) working observations&nbsp;	\(\frac{\phi}{W_i}\)	
\(\underline{\beta}\) with working weights&nbsp;	\(\underline{\beta} = (X^TWX)^{-1}X^TW\underline{z}\)	
Hat matrix with working weights&nbsp;	\( H = W^{\frac{1}{2}}X(X^TWX)^{-1}X^TW^{\frac{1}{2}}\)	
Standardised Working residuals&nbsp;	\(r_i ' = \frac{r_i}{\sqrt{1-h_i}}\)	
Standardised Deviance Residuals&nbsp;	\( r_i ' = sign(y_i - \hat{\mu}_i)\sqrt{\frac{\omega_i d(y_i, \hat{\mu}_i)}{1-h_i}}\)	
Standardised Pearson Residuals&nbsp;	\( r_i ' = \frac{y_i - \hat{\mu}_i}{\sqrt{(1-h_i)\frac{V(\hat{\mu}_i)}{\omega_i}}}\)	
Logistic Regression model: EDM and link function&nbsp;	EDM: Binomial<div>Link function: Logit</div>	
Logistic regression<div>For \(log(\frac{\mu}{1-\mu}) = \alpha + \beta x \), odds of \(x_i = 0 \)</div>	\(\frac{\mathbb{P}(Y_i = 1 | x_i = 0)}{\mathbb{P}(Y_i = 0 | x_i = 0)} = e^{\alpha}\)	
Logistic regression<div>For \(log(\frac{\mu}{1-\mu}) = \alpha + \beta x \), odds of \(x_i = 1 \)</div>	\(\frac{\mathbb{P}(Y_i = 1 | x_i = 1)}{\mathbb{P}(Y_i = 0 | x_i = 1)} = e^{\alpha + \beta}\)	
Logistic regression<div>For \(log(\frac{\mu}{1-\mu}) = \alpha + \beta x \), odds ratio</div>	\(\frac{\mathbb{P}(Y_i = 1 | x_i = 1)}{\mathbb{P}(Y_i = 0 | x_i = 1)}\frac{\mathbb{P}(Y_i = 0 | x_i = 0)}{\mathbb{P}(Y_i = 1 | x_i = 0)}&nbsp;= e^{\beta}\)	
Logistic Regression model for factor variable \(x_i \in \{1,2,...,K\}\)	\(log(\frac{\mu_i}{1-\mu_i}) = \alpha + \Sigma_{k=2}^{K} \beta_k I_\{x_i = k\} \)	
Log odds for \(x_i = k\)&nbsp;	\(\frac{\mathbb{P}(Y_i = 1 | x_i = k)}{\mathbb{P}(Y_i = 0 | x_i = k)} = e^{\alpha + \beta k}\)	
Odds ratio for level k vs. level 1	\(\frac{\mathbb{P}(Y_i = 1 | x_i = k)}{\mathbb{P}(Y_i = 0 | x_i = k)}\frac{\mathbb{P}(Y_i = 0 | x_i = 1)}{\mathbb{P}(Y_i = 1 | x_i = 1)}&nbsp;= e^{\beta k}\)	
Probit link function's corresponding tolerance distribution&nbsp;	Normal	
Logit link function's corresponding tolerance distribution&nbsp;	Logistic	
cloglog link function's corresponding tolerance distribution&nbsp;	Gumbel&nbsp;	
Cauchit link function's corresponding tolerance distribution&nbsp;	Cauchy	
Log link function's corresponding tolerance distribution&nbsp;	Exponential&nbsp;	
Design Matrix Defintition of Nested Models	Consider two models<div><br></div><div>\[ M_1 : \mathbb{E}(Y) = X^{(1)}\beta^{(1)}\\ M_2 : \mathbb{E}(Y) = X^{(2)}\beta^{(2)} \]</div><div><br></div><div>\(M_1 \preceq M_2\) if there is a matrix \(C\) such that \(X^{(1)} = X^{(2)}C\). If \(C\) is invertible then the models are identical (though possibly parametrised differently).</div>	definition
Deviance	The deviance is the sum of squares evaluated at \(\hat\beta\).<div><br></div><div>\[S(\hat\beta) = \sum(y_i - x_i^T \hat\beta)^2 = \sum(y_i - \hat{y_i})^2 \]</div>	definition
Coefficient of Determination	\[R^2 = 1 - \frac{\text{Deviance}(M)}{\text{Deviance}(M_0)} =&nbsp;&nbsp;1 - \frac{\sum(y_i - \hat{y_i})^2}{\sum(y_i - \bar{y})^2}\]<div><br></div><div>In terms of sum of squares, we have</div><div><br></div><div>\[R^2 = 1 - \frac{\text{ResidSS}}{\text{TotalSS}} = \frac{\text{RegrSS}}{\text{TotalSS}}\]</div>	definition
Decomposition of Sum of Squares	\[\sum(y_i - \bar{y})^2 = \sum(y_i - \hat{y_i})^2 + \sum(\hat{y_i} - \bar{y})^2\]<div><br></div><div>In other words, TotalSS = ResidualSS + RegressionSS.</div>	result
Adjusted R-Squared (and Justification)	\[R^2_\text{adj} = 1 - \frac{\frac{1}{n-p}\text{ResidSS}}{\frac{1}{n-1}\text{TotalSS}}\]	definition
Mallows' \(C_p\)	Consider a model \(M \preceq M_\text{max}\),<div><br></div><div>\[C_p(M) = \frac{\text{Deviance}(M)}{s^2_\text{max}} + 2p -&nbsp; n\]</div><div><br></div><div>A smaller \(C_p\) is better.</div>	definition
LOOCV results	\[y_i - \hat{\beta_{(-i)}}^T = \hat{\varepsilon_i} + \frac{h_i \hat{\varepsilon_i}}{1 - h_i} = \hat{\varepsilon_i} + O(n^{-1})\]<div><br></div><div>\[\sum(y_i - \hat{\beta_{(-i)}}^T)^2 \overset{\mathbb{E}}\longrightarrow \text{Deviance} + 2\sigma^2p\]</div>	result
The Marginality Principle	If you include an interaction term in a linear model, you should also include the main effects. You should also be cautious when testing or interpreting effects of main effects.	definition
Martin's Advice on Interactions	<ul><li>Get the scale of dose-response relationship right first</li><li>Respect the marginality principle</li><li>At least variable in an interaction term should be a factor</li><li>Use the stratified parameterization</li><li>Do not look for interactions without good evidence</li><li>Do not go beyond two-way interations</li></ul>	definition
Akaike Information Criterion	\[\text{AIC} = -2\mathcal{l}(\hat\beta, \hat{\sigma^2}) + 2p\]<div><br></div><div>(\(\mathcal{l}\) is log-likelihood)</div>	definition
AIC for Normal Linear Model	<div>Fixed variance \(\sigma^2\):</div><div><br></div>\[\frac{\sum(y_i - \hat\beta^Tx_i)^2}{\sigma^2} + 2p\]<div><br></div><div>Unknown variance using MLE:</div><div><br></div><div>\[n\log(\text{ResidSS}) + 2p\]<br></div>	result
F Statistic for Comparison to Null Model	\[F = \frac{\frac{1}{p-1}\text{RegrSS}}{\frac{1}{n-p}\text{ResidSS}} = \frac{\frac{1}{p-1}\sum\left(\hat{y_i} - \bar{y}\right)^2}{\frac{1}{n-p_2}\sum\left(\bar{y} - y_i\right)^2}\sim F_{p-1, n-p}\]	result
F Statistic for Comparison of Nested Models	For \(M_1 \preceq M_2\),<div><br></div><div>\[F = \frac{\frac{1}{p_2-p_1}\sum\left(\hat{y_i}^{(1)} - \hat{y_i}^{(2)}\right)^2}{\frac{1}{n-p_2}\sum\left(\hat{y_i}^{(2)} - y_i\right)^2} \sim F_{p_2-p_1, n-p_2}\]</div>	result
Poisson Distribution Properties	\[p(y | \mu) = \frac{e^{-\mu}\mu^y}{y!} \\ Y_+ \sim \text{Pois}(\lambda T_+)\\M(t) = \exp(\mu(e^t-1)) \\ \hat\lambda = \frac{y}{T}\]	result
Binomial Distribution Properties	\[M(t) = \left(\mu e^t + 1 - \mu\right)^N\]	result
Exponential Dispersion Model	A random variable is distributed according to to an exponential dispersion model if we can write its probability density function as<div><br></div><div>\[ p(y | \theta, \phi) = a(y, \phi) \exp\left(\frac{\theta y - b(\theta)}{\phi}\right) \]</div><div><br></div><div>\(\theta \in \Theta \subseteq \mathbb{R}\) is called the canonical parameter and \(\phi &gt; 0\) is the dispersion parameter (which may be fixed or free).</div>	definition
Normal Distribution EDM parameters	\[\theta = \mu \\ \phi = \sigma^2 \\ b(\theta) = \frac{\theta ^ 2}{2}\]	result
Poisson Distribution EDM parameters	\[\theta = \log\mu \\ \phi = 1 \\ b(\theta) = e^\theta\]	result
Binomial Distribution EDM parameters	\[\theta = \text{logit}(\mu) = \log\left(\frac{\mu}{1-\mu}\right)\\ \phi = 1 \\ b(\theta) = N\log(1 + e^\theta) \]	result
Scaled Binomial Distribution	Let \(Y \sim \text{Bin}(N, \mu)\) and \(Z = \frac{Y}{N}\). Then \(Z \sim \text{ScaledBin}(N, \mu)\).	definition
Scaled Binomial Distribution EDM parameters	\[\theta = \text{logit}(\mu) = \log\left(\frac{\mu}{1-\mu}\right)\\ \phi = \frac{1}{N} \\ b(\theta) = \log(1 + e^\theta) \]	result
MGF of EDMs	\[M(t) = \exp\left(\frac{b(\theta + t\phi - b(\theta))}{\phi}\right)\]	result
Expectation and Variance of EDMs	(Calculated through CGF)<div><br></div><div>\[ \mathbb{E}(Y) = b'(\theta) \\ \mathbb{V}(Y) = \phi b''(\theta) \]</div><div><br></div><div>(Implies that \(\theta, \mu\) have a 1-1 correspondence)</div>	result
Variance Function	The variance function of an EDM is that function \(V\) satisfying \(\mathbb{V}(Y) = \phi V(\mu)\).	definition
Uniqueness of EDM	An EDM is uniquely defined by the relation \(\mathbb{V}(Y) = \phi V(\mu)\).<div><br></div><div>\[\theta(\mu) = \int\frac{1}{V(\mu)} d\mu\]</div>	definition